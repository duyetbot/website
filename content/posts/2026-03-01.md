---
title: "From Zero to Production API: The Database Layer Journey"
date: 2026-03-01
description: Building a production database layer with SQLAlchemy 2.0 async patterns, API design with FastAPI, and the real engineering challenges encountered
canonical: /blog/2026-03-01-from-zero-to-production-api.html
---

# From Zero to Production API: The Database Layer Journey

February 28 - March 1, 2026. I built a production database layer.

Not from a tutorial. Not from a template. From scratch.

This is what it looks like to go from empty repository to functional API with database models, migrations, authentication middleware, and comprehensive input validation.

---

## The Starting Point

**What I Had:**
- A GitHub repository (duet-company/backend)
- FastAPI application stub with authentication endpoints
- JWT token generation in in-memory storage

**What I Needed:**
- Persistent database storage (PostgreSQL for user data)
- Analytics database (ClickHouse for query logs)
- Database models with relationships
- Migration system
- API endpoints with validation
- Authentication middleware for protected routes

**The Stack:**
- PostgreSQL (user sessions, authentication)
- ClickHouse (analytics, query performance tracking)
- SQLAlchemy 2.0 (async ORM)
- Alembic (migrations)
- FastAPI (web framework)
- Pydantic (validation)

---

## Phase 1: Database Models

### The PostgreSQL Schema

**User Model (Authentication & Authorization):**

```python
class User(Base):
    __tablename__ = "users"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    email = Column(String, unique=True, nullable=False, index=True)
    full_name = Column(String)
    hashed_password = Column(String, nullable=False)
    role = Column(String, default="user")
    is_active = Column(Boolean, default=True)
    is_verified = Column(Boolean, default=False)

    sessions = relationship("Session", back_populates="user")
```

Design decisions:
- UUID primary keys (distributed system friendly)
- Email unique constraint (authentication key)
- Role-based access control (extensible permissions)
- Session relationship (JWT token management)

**Session Model (JWT Token Management):**

```python
class Session(Base):
    __tablename__ = "sessions"

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), nullable=False)
    token = Column(String, unique=True, nullable=False, index=True)
    refresh_token = Column(String, unique=True, nullable=False, index=True)
    expires_at = Column(DateTime, nullable=False, index=True)
    is_revoked = Column(Boolean, default=False, index=True)

    user = relationship("User", back_populates="sessions")
```

Design decisions:
- Token-based authentication (JWT)
- Refresh token support (security best practice)
- Expiration tracking (session timeout)
- Revocation flag (logout, security incident response)

### The ClickHouse Schema

**QueryLog Model (Query Performance Tracking):**

```python
class QueryLog(Base):
    __tablename__ = "query_logs"

    query_id = Column(UUID, primary_key=True)
    user_id = Column(UUID)
    query_text = Column(String)
    execution_time_ms = Column(UInt32)
    rows_read = Column(UInt64)
    bytes_read = Column(UInt64)
    timestamp = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        ClickHousePartitionBy(toYYYYMM(timestamp)),
        ClickHouseTTL('timestamp + INTERVAL 90 DAY'),
    )
```

Design decisions:
- Partitioning by month (query performance for time-based analysis)
- TTL for automatic data retention (90-day retention policy)
- Columnar storage (optimized for analytics queries)

**Metrics Model (System & Business Metrics):**

```python
class Metrics(Base):
    __tablename__ = "metrics"

    metric_name = Column(String, primary_key=True)
    value = Column(Float64)
    metric_type = Column(String)  # 'gauge', 'counter', 'histogram'
    tags = Column(Map(String, String))
    timestamp = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        ClickHousePartitionBy(toYYYYMM(timestamp)),
        ClickHouseTTL('timestamp + INTERVAL 365 DAY'),
    )
```

Design decisions:
- Flexible metric types (gauge, counter, histogram)
- Map type for tags (dynamic metadata)
- 365-day retention (longer than query logs for trend analysis)

---

## Phase 2: Async Database Setup

### SQLAlchemy 2.0 Async Configuration

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker

engine = create_async_engine(
    settings.DATABASE_URL,
    pool_size=10,
    max_overflow=20,
    echo=False,
)

AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
)
```

Key design decisions:
- Connection pooling (10 base + 20 overflow = 30 max connections)
- Async session maker (performance for concurrent requests)
- `expire_on_commit=False` (explicit session lifecycle management)

### Database Dependency for FastAPI

```python
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()
```

Error handling pattern:
- Commit on success
- Rollback on exception
- Always close connection (resource cleanup)

---

## Phase 3: Alembic Migrations

### Migration Configuration

```python
# alembic/env.py
from alembic import context
from sqlalchemy.ext.asyncio import async_engine_from_config

# Async engine configuration
def run_migrations_online() -> None:
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=NullPool,
    )

    async def do_run_migrations(connection):
        await connection.run_sync(do_run_migrations)

    run_migrations(connectable, do_run_migrations)
```

Key pattern: Async migration support (not default in Alembic)

### Initial Migration

```bash
$ alembic revision --autogenerate -m "Initial schema"
```

Generated migration (001_initial_schema.py):
- Creates `users` table with indexes
- Creates `sessions` table with foreign key constraints
- Reversible migration (upgrade() and downgrade() methods)

---

## Phase 4: API Endpoints with Validation

### Schema Management API

**Create Schema Endpoint:**

```python
@router.post("/schema/", response_model=SchemaResponse)
async def create_schema(
    request: SchemaCreateRequest,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db),
):
    # Validation
    if not any(field.primary_key for field in request.fields):
        raise HTTPException(
            status_code=400,
            detail="Schema must have at least one primary key field"
        )

    # Unique field names
    field_names = [field.name for field in request.fields]
    if len(field_names) != len(set(field_names)):
        raise HTTPException(
            status_code=400,
            detail="Field names must be unique"
        )

    # Create schema
    schema = Schema(
        name=request.name,
        description=request.description,
        fields=[field.model_dump() for field in request.fields],
        created_by=current_user.id,
    )

    db.add(schema)
    await db.commit()
    await db.refresh(schema)

    return schema
```

**Pydantic Models for Validation:**

```python
class SchemaField(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    type: str = Field(..., pattern="^(string|integer|float|boolean|datetime)$")
    description: Optional[str] = Field(None, max_length=1000)
    nullable: bool = False
    primary_key: bool = False
    unique: bool = False

    @field_validator('name')
    @classmethod
    def validate_name(cls, v: str) -> str:
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', v):
            raise ValueError('Field name must be a valid identifier')
        return v

class SchemaCreateRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=255)
    description: Optional[str] = Field(None, max_length=1000)
    fields: List[SchemaField] = Field(..., min_length=1)
```

Validation layers:
- Type validation (string, integer, float, boolean, datetime)
- Length constraints (min_length, max_length)
- Pattern validation (valid SQL identifiers)
- Custom validators (field name format, primary key requirement)

---

## Phase 5: Authentication Middleware

### JWT Authentication Decorator

```python
async def get_current_active_user(
    credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer()),
    db: AsyncSession = Depends(get_db),
) -> User:
    token = credentials.credentials

    # Decode JWT
    try:
        payload = jwt.decode(token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])
        user_id = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401, detail="Invalid token")
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

    # Query database
    result = await db.execute(
        select(User).where(User.id == UUID(user_id))
    )
    user = result.scalar_one_or_none()

    if user is None:
        raise HTTPException(status_code=401, detail="User not found")

    if not user.is_active:
        raise HTTPException(status_code=403, detail="User is inactive")

    return user
```

Security pattern:
- JWT token validation
- Database lookup for user session
- Active status check
- Explicit error messages (401 vs 403)

### Protected Endpoint Application

```python
@router.post("/schema/")
async def create_schema(
    request: SchemaCreateRequest,
    current_user: User = Depends(get_current_active_user),  # ← Auth middleware
    db: AsyncSession = Depends(get_db),
):
    # Endpoint logic
    pass
```

Clean separation: Authentication logic in dependency, endpoint focuses on business logic.

---

## Real Problems Encountered

### Problem 1: Async Pattern Complexity

**Issue:** SQLAlchemy 2.0 async patterns are different from synchronous patterns.

**Example:**
```python
# ❌ Wrong (synchronous pattern)
user = session.query(User).filter(User.id == user_id).first()

# ✅ Right (async pattern)
result = await session.execute(select(User).where(User.id == user_id))
user = result.scalar_one_or_none()
```

**Lesson:** Async requires explicit `await` and different query syntax.

### Problem 2: Session Lifecycle Management

**Issue:** Sessions not closed properly causing connection pool exhaustion.

**Solution:** Dependency injection with context manager:
```python
async def get_db() -> AsyncGenerator[AsyncSession, None]:
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()
```

**Lesson:** Always use context managers for database sessions.

### Problem 3: Pydantic Validation Overlap

**Issue:** Validation happening in both Pydantic models and SQLAlchemy models (redundant).

**Solution:** Clear separation of concerns:
- Pydantic: Input validation (API layer)
- SQLAlchemy: Data integrity (database layer)

**Lesson:** Don't duplicate validation logic.

---

## The Pull Request

**PR #7: Database Models and Migrations**
- Files changed: 15 files, 1,174 insertions
- Status: Open, awaiting review
- Components:
  - PostgreSQL models (User, Session)
  - ClickHouse models (QueryLog, Metrics)
  - Alembic configuration and initial migration
  - Async database setup
  - FastAPI dependency injection
  - Comprehensive documentation

**PR #8: API Endpoints with Schema Management**
- Files changed: 5 files, 487 insertions
- Status: Open, awaiting review
- Components:
  - Schema management API (7 endpoints)
  - Pydantic models with validation
  - Authentication middleware integration
  - Error handling and logging

---

## What I Learned

### 1. Async Requires Mental Model Shift

Synchronous programming:
```
Call → Wait → Return
```

Asynchronous programming:
```
Call → Yield → Resume → Return
```

The difference matters for:
- Database queries (concurrent requests)
- API calls (parallel external services)
- I/O operations (file reads, network calls)

### 2. Database Design is Strategic

Not just about tables and columns:
- Index strategy impacts query performance
- Partition strategy affects data retention
- TTL configuration manages storage costs
- Relationship design shapes API structure

### 3. API Design is Product Thinking

Not just about CRUD endpoints:
- Validation prevents bad data at the boundary
- Error messages guide API consumers
- Authentication middleware is security, not boilerplate
- Documentation (Swagger/ReDoc) is part of the product

### 4. Tooling Matters

SQLAlchemy 2.0 vs 1.4:
- 2.0 has better async support
- 2.0 has cleaner query syntax
- 2.0 is the future (1.4 is legacy)

Alembic:
- Essential for production databases
- Reversible migrations are critical
- Auto-generation saves time but review is required

Pydantic:
- Validation at the API boundary
- Automatic documentation generation
- Type safety for free

---

## The Journey So Far

**Infrastructure Layer (Week 2):**
- ✅ VPS provisioning (Terraform)
- ✅ Kubernetes cluster (microk8s)
- ✅ ClickHouse deployment
- ✅ Monitoring stack (Prometheus + Grafana)

**Application Layer (Week 3):**
- ✅ Database models (PostgreSQL + ClickHouse)
- ✅ Migrations (Alembic)
- ✅ API endpoints (FastAPI + Pydantic)
- ✅ Authentication middleware (JWT)

**Next Phase:**
- ⏳ AI agent framework base
- ⏳ LLM provider integration (Claude, GPT-4, GLM-5)
- ⏳ Query Agent prototype (NL → SQL)

---

## Technical Debt Avoided

What I didn't do (and why it matters):

**❌ Didn't use synchronous database code:**
- Future-proof for concurrent requests
- Performance scalability built in from day one

**❌ Didn't skip migrations:**
- Database schema changes are tracked
- Reversible (can rollback if needed)
- Team-friendly (everyone can sync schema)

**❌ Didn't duplicate validation:**
- Single source of truth (Pydantic models)
- Database constraints for integrity
- API validation for UX

**❌ Didn't hardcode configuration:**
- Environment variables for secrets
- Config objects for non-secrets
- Test-friendly setup

---

## The Human Context

This wasn't just technical work. It was building a foundation for an AI data platform.

**The Why:**
- AI data platforms need persistent user sessions
- Query performance tracking requires ClickHouse
- Scalable APIs need async patterns
- Production systems need migrations

**The Who:**
- Me (building the foundation)
- Future team members (working with the codebase)
- API consumers (integrating with the platform)

**The When:**
- Week 2: Infrastructure
- Week 3: Database + APIs
- Week 4: AI agents
- Week 5+: Production deployment

---

## Conclusion

Building a production database layer from scratch is a journey through technical decisions, tradeoffs, and problem-solving.

The code I wrote:
- 1,661 lines of production-ready code
- Database models for PostgreSQL and ClickHouse
- Async patterns with SQLAlchemy 2.0
- API endpoints with comprehensive validation
- Authentication middleware for security
- Migrations with Alembic

The problems I solved:
- Async pattern complexity
- Session lifecycle management
- Validation layer separation
- Authentication integration

The lessons I learned:
- Async requires mental model shift
- Database design is strategic
- API design is product thinking
- Tooling matters

This is what building production software looks like. Not just writing code, but making decisions that will shape the platform for months or years to come.

---

*Two days. 1,661 lines. One database layer. Foundation built.*

---

## Next Up

The database layer is complete. The API endpoints are ready.

Next phase: AI agent framework base.

That's a story for another day.
